import csv
import os
import sys
import zipfile
import io

def distill_wfo():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    root_dir = os.path.dirname(script_dir)
    INPUT_DIR = path_join = os.path.join(root_dir, 'data', 'input')
    TEMP_DIR = os.path.join(root_dir, 'data', 'temp')
    OUTPUT_FILE = os.path.join(TEMP_DIR, 'wfo_family_order_map.csv')
    
    # Target files
    target_csv = 'classification.csv'
    
    # 1. Locate source
    source_path = None
    zip_path = None
    
    if os.path.exists(os.path.join(INPUT_DIR, target_csv)):
        source_path = os.path.join(INPUT_DIR, target_csv)
    else:
        # Check ZIPs
        for f in os.listdir(INPUT_DIR):
            if f.lower().endswith('.zip'):
                p = os.path.join(INPUT_DIR, f)
                with zipfile.ZipFile(p, 'r') as z:
                    if target_csv in z.namelist():
                        zip_path = p
                        break
    
    if not source_path and not zip_path:
        print(f"Error: Could not find '{target_csv}' in {INPUT_DIR}")
        sys.exit(1)

    print(f"Distilling WFO Backbone...")
    
    # Data structures
    family_links = {} # parentID -> list of family names
    order_names = {} # taxonID -> scientificName
    
    def process_stream(f):
        # Increase field size for large remarks
        csv.field_size_limit(sys.maxsize)
        reader = csv.DictReader(f, delimiter='\t' if source_path and source_path.endswith('.txt') else '\t')
        # Check for tab or comma
        if not reader.fieldnames or 'taxonID' not in reader.fieldnames:
             f.seek(0)
             reader = csv.DictReader(f, delimiter=',')
             
        row_count = 0
        for row in reader:
            row_count += 1
            rank = row.get('taxonRank', '').lower()
            t_id = row.get('taxonID')
            p_id = row.get('parentNameUsageID')
            name = row.get('scientificName')
            
            if rank == 'family':
                if p_id not in family_links: family_links[p_id] = []
                family_links[p_id].append(name)
            
            # Cache potential orders (records that are parents of families)
            # We don't know if it's an order yet, but if it matches a family's p_id, we'll check later.
            order_names[t_id] = name
            
            if row_count % 100000 == 0:
                print(f"  Analyzed {row_count:,} records...")

    # PASS 1 & 2 combined via caching (since orders are usually processed before families)
    if zip_path:
        with zipfile.ZipFile(zip_path, 'r') as z:
            with io.TextIOWrapper(z.open(target_csv), encoding='utf-8') as f:
                process_stream(f)
    else:
        with open(source_path, 'r', encoding='utf-8') as f:
            process_stream(f)

    # Resolve and Write
    print(f"Resolving {len(family_links)} family-to-parent links...")
    final_map = []
    for p_id, families in family_links.items():
        order_name = order_names.get(p_id, "Unknown Order")
        for fam in families:
            final_map.append({'family': fam, 'order': order_name})

    with open(OUTPUT_FILE, 'w', encoding='utf-8', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=['family', 'order'])
        writer.writeheader()
        writer.writerows(final_map)

    print(f"Success! Distilled {len(final_map)} family/order pairs to {OUTPUT_FILE}")

if __name__ == "__main__":
    distill_wfo()