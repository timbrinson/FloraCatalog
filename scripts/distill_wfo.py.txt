import csv
import os
import sys
import zipfile
import io

def distill_wfo():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    root_dir = os.path.dirname(script_dir)
    INPUT_DIR = os.path.join(root_dir, 'data', 'input')
    TEMP_DIR = os.path.join(root_dir, 'data', 'temp')
    
    # Single Output consistent with WCVP naming
    OUTPUT_FILE = os.path.join(TEMP_DIR, 'wfo_import.csv')
    
    # Target file in WFO archive
    target_csv = 'classification.csv'
    
    # Standard WFO Column Set (29 columns as provided by user)
    WFO_COLUMNS = [
        'taxonID', 'scientificNameID', 'localID', 'scientificName', 'taxonRank', 
        'parentNameUsageID', 'scientificNameAuthorship', 'family', 'subfamily', 
        'tribe', 'subtribe', 'genus', 'subgenus', 'specificEpithet', 
        'infraspecificEpithet', 'verbatimTaxonRank', 'nomenclaturalStatus', 
        'namePublishedIn', 'taxonomicStatus', 'acceptedNameUsageID', 
        'originalNameUsageID', 'nameAccordingToID', 'taxonRemarks', 'created', 
        'modified', 'references', 'source', 'majorGroup', 'tplID'
    ]

    # 1. Locate source
    source_path = None
    zip_path = None
    
    if os.path.exists(os.path.join(INPUT_DIR, target_csv)):
        source_path = os.path.join(INPUT_DIR, target_csv)
    else:
        if os.path.exists(INPUT_DIR):
            for f in os.listdir(INPUT_DIR):
                if f.lower().endswith('.zip'):
                    p = os.path.join(INPUT_DIR, f)
                    try:
                        with zipfile.ZipFile(p, 'r') as z:
                            if target_csv in z.namelist():
                                zip_path = p
                                break
                    except Exception:
                        continue
    
    if not source_path and not zip_path:
        print(f"Error: Could not find '{target_csv}' in {INPUT_DIR}")
        sys.exit(1)

    print(f"Distilling WFO Backbone (Filter: Higher Ranks only for Storage Efficiency)...")
    
    def process_stream(f):
        csv.field_size_limit(sys.maxsize)
        sample = f.readline()
        f.seek(0)
        delimiter = '\t' if '\t' in sample else ','
        
        reader = csv.DictReader(f, delimiter=delimiter)
        
        row_count = 0
        extracted_count = 0
        
        # Prepare Analysis File writer
        with open(OUTPUT_FILE, 'w', encoding='utf-8', newline='') as of:
            writer = csv.DictWriter(of, fieldnames=WFO_COLUMNS, extrasaction='ignore')
            writer.writeheader()

            for row in reader:
                row_count += 1
                
                # V2.33.10: This application requires the Supabase Pro Plan.
                # To optimize Pro Plan storage utilization, we exclude Genus and below.
                # All ranks above Genus in WFO have an empty 'genus' literal column.
                # This preserves Superorders, Clades, and intermediate ranks needed for gap closure.
                if not row.get('genus'):
                    writer.writerow(row)
                    extracted_count += 1
                
                if row_count % 100000 == 0:
                    print(f"  Processed {row_count:,} records...")

        print(f"Success!")
        print(f"  Generated filtered backbone staging file: {OUTPUT_FILE} ({extracted_count:,} rows)")

    if zip_path:
        with zipfile.ZipFile(zip_path, 'r') as z:
            with io.TextIOWrapper(z.open(target_csv), encoding='utf-8', errors='replace') as f:
                process_stream(f)
    else:
        with open(source_path, 'r', encoding='utf-8', errors='replace') as f:
            process_stream(f)

if __name__ == "__main__":
    distill_wfo()