import csv
import os
import sys
import zipfile
import io

def distill_wfo():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    root_dir = os.path.dirname(script_dir)
    INPUT_DIR = os.path.join(root_dir, 'data', 'input')
    TEMP_DIR = os.path.join(root_dir, 'data', 'temp')
    OUTPUT_FILE = os.path.join(TEMP_DIR, 'wfo_family_order_map.csv')
    
    # Target files
    target_csv = 'classification.csv'
    
    # 1. Locate source
    source_path = None
    zip_path = None
    
    if os.path.exists(os.path.join(INPUT_DIR, target_csv)):
        source_path = os.path.join(INPUT_DIR, target_csv)
    else:
        # Check ZIPs
        if os.path.exists(INPUT_DIR):
            for f in os.listdir(INPUT_DIR):
                if f.lower().endswith('.zip'):
                    p = os.path.join(INPUT_DIR, f)
                    try:
                        with zipfile.ZipFile(p, 'r') as z:
                            if target_csv in z.namelist():
                                zip_path = p
                                break
                    except Exception:
                        continue
    
    if not source_path and not zip_path:
        print(f"Error: Could not find '{target_csv}' in {INPUT_DIR}")
        sys.exit(1)

    print(f"Distilling WFO Backbone...")
    
    # Data structures
    family_links = {} # parentID -> list of family names
    order_names = {} # taxonID -> scientificName
    
    def process_stream(f):
        # Increase field size for large remarks
        csv.field_size_limit(sys.maxsize)
        
        # Darwin Core Archives typically use Tabs, but some versions use Comma.
        # We attempt to detect the header and set delimiter.
        sample = f.readline()
        f.seek(0)
        delimiter = '\t' if '\t' in sample else ','
        
        reader = csv.DictReader(f, delimiter=delimiter)
             
        row_count = 0
        for row in reader:
            row_count += 1
            rank = row.get('taxonRank', '').lower()
            t_id = row.get('taxonID')
            p_id = row.get('parentNameUsageID')
            name = row.get('scientificName')
            
            if not t_id or not name:
                continue

            if rank == 'family':
                if p_id not in family_links: family_links[p_id] = []
                family_links[p_id].append(name)
            
            # Cache potential orders (records that are parents of families)
            order_names[t_id] = name
            
            if row_count % 100000 == 0:
                print(f"  Analyzed {row_count:,} records...")

    # Open with errors='replace' to handle messy encoding in the ~1GB file
    if zip_path:
        with zipfile.ZipFile(zip_path, 'r') as z:
            with io.TextIOWrapper(z.open(target_csv), encoding='utf-8', errors='replace') as f:
                process_stream(f)
    else:
        with open(source_path, 'r', encoding='utf-8', errors='replace') as f:
            process_stream(f)

    # Resolve and Write
    print(f"Resolving {len(family_links)} family-to-parent links...")
    final_map = []
    for p_id, families in family_links.items():
        order_name = order_names.get(p_id, "Unknown Order")
        for fam in families:
            final_map.append({'family': fam, 'order': order_name})

    with open(OUTPUT_FILE, 'w', encoding='utf-8', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=['family', 'order'])
        writer.writeheader()
        writer.writerows(final_map)

    print(f"Success! Distilled {len(final_map)} family/order pairs to {OUTPUT_FILE}")

if __name__ == "__main__":
    distill_wfo()